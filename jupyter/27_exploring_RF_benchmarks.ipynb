{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Performance Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.irf_utils' from '/home/runjing_liu/Documents/iRF/scikit-learn-sandbox/jupyter/utils/irf_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from IPython.display import display, Image\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Import our custom utilities\n",
    "from imp import reload\n",
    "from utils import irf_jupyter_utils\n",
    "from utils import irf_utils\n",
    "from utils import iRF_benchmarks_lib\n",
    "reload(irf_jupyter_utils)\n",
    "reload(irf_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_data = load_breast_cancer()              \n",
    "responses = raw_data.target\n",
    "features = raw_data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call function to compute benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_trials = 10 # number of times to run random forest for our benchmarks\n",
    "n_estimators = 20 # number of trees in the random forest\n",
    "train_split_propn = 0.8\n",
    "\n",
    "metrics_all, metrics_summary, feature_importances = \\\n",
    "            iRF_benchmarks_lib.RF_benchmarks(features, responses, \n",
    "                                    n_trials = n_trials,\n",
    "                                    train_split_propn = train_split_propn, \n",
    "                                    n_estimators=n_estimators,\n",
    "                                    seed = 2017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['precision_score', 'confusion_matrix', 'f1_score', 'hamming_loss', 'recall_score', 'log_loss', 'accuracy_score', 'zero_one_loss', 'time'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_summary.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of full dataset (#samples , # features):  (569, 30)\n",
      "Number of training samples:  455.0\n",
      "Number of test samples:  114.0\n",
      "number of trees in the random forest:  20\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of full dataset (#samples , # features): ', np.shape(features))\n",
    "print('Number of training samples: ', np.round(np.shape(features)[0] * train_split_propn))\n",
    "print('Number of test samples: ', np.round(np.shape(features)[0]*(1-train_split_propn)))\n",
    "print('number of trees in the random forest: ', n_estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing and some accuracy scores across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time (seconds) to compute RF [mean, std]:  [0.038167762756347659, 0.0056543663450365289]\n",
      "accuracy_score [mean, std]:  [0.95350877192982464, 0.013040411181858341]\n",
      "hammming_loss [mean, std]:  [0.04649122807017543, 0.013040411181858337]\n"
     ]
    }
   ],
   "source": [
    "print('time (seconds) to compute RF [mean, std]: ', metrics_summary['time'])\n",
    "print('accuracy_score [mean, std]: ', metrics_summary['accuracy_score'])\n",
    "print('hammming_loss [mean, std]: ', metrics_summary['hamming_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look the stability of feature importances across trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top five feature importances across trials\n",
      "trial0:  [20 23 27 22  0]\n",
      "trial1:  [27 22  7 23 20]\n",
      "trial2:  [22 20 23  7  3]\n",
      "trial3:  [20 23  2 22 27]\n",
      "trial4:  [27 22  7  0 20]\n",
      "trial5:  [23  7 22  3 20]\n",
      "trial6:  [23 27 22  0 13]\n",
      "trial7:  [23 20 27  7  6]\n",
      "trial8:  [27 20 23 22  3]\n",
      "trial9:  [23 27 20  7  3]\n"
     ]
    }
   ],
   "source": [
    "print('top five feature importances across trials')\n",
    "\n",
    "for i in range(n_trials): \n",
    "    # sort by feature importance\n",
    "    importances_rank = np.argsort(feature_importances[i])[::-1]\n",
    "    print('trial' + str(i) + ': ', importances_rank[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## iRF benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call function to compute iRF benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_trials = 5 # number of times to run iRF in benchmarking\n",
    "\n",
    "# parameters for iRF\n",
    "train_split_propn = 0.8\n",
    "n_estimators = 20\n",
    "B = 20\n",
    "M = 20\n",
    "max_depth = 5\n",
    "n_estimators_bootstrap = 5\n",
    "\n",
    "metrics_all_iRF, metrics_summary_iRF, stability_all, feature_importances_iRF = \\\n",
    "    iRF_benchmarks_lib.iRF_benchmarks(features, responses, n_trials = n_trials,\n",
    "                                    K=5,\n",
    "                                    train_split_propn = train_split_propn,\n",
    "                                    n_estimators=n_estimators,\n",
    "                                    B=B,\n",
    "                                    propn_n_samples=.2,\n",
    "                                    bin_class_type=1,\n",
    "                                    M=M,\n",
    "                                    max_depth=max_depth,\n",
    "                                    noisy_split=False,\n",
    "                                    num_splits=2,\n",
    "                                    n_estimators_bootstrap=n_estimators_bootstrap, \n",
    "                                    seed = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relevant parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of full dataset (#samples , # features):  (569, 30)\n",
      "Number of training samples:  455.0\n",
      "Number of test samples:  114.0\n",
      "\n",
      "\n",
      "number of trees in full random forest:  20\n",
      "number of bootstrap samples:  20\n",
      "number of trees in RIT:  20\n",
      "max depth of RIT:  5\n",
      "number of trees is RF bootstrap:  5\n"
     ]
    }
   ],
   "source": [
    "print('Dimensions of full dataset (#samples , # features): ', np.shape(features))\n",
    "print('Number of training samples: ', np.round(np.shape(features)[0] * train_split_propn))\n",
    "print('Number of test samples: ', np.round(np.shape(features)[0]*(1-train_split_propn)))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('number of trees in full random forest: ', n_estimators)\n",
    "print('number of bootstrap samples: ', B)\n",
    "print('number of trees in RIT: ', M)\n",
    "print('max depth of RIT: ', max_depth)\n",
    "print('number of trees is RF bootstrap: ', n_estimators_bootstrap)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing and some accuracy scores across trials\n",
    "These are metrics on the random forest at iteration K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time (seconds) to compute iRF [mean, std]:  [2.640289306640625, 0.10714668470600612]\n",
      "\n",
      "\n",
      "accuracy_score [mean, std]:  [0.93859649122807021, 0.012405382126079773]\n",
      "hammming_loss [mean, std]:  [0.061403508771929814, 0.012405382126079782]\n"
     ]
    }
   ],
   "source": [
    "print('time (seconds) to compute iRF [mean, std]: ', metrics_summary_iRF['time'])\n",
    "print('\\n')\n",
    "print('accuracy_score [mean, std]: ', metrics_summary_iRF['accuracy_score'])\n",
    "print('hammming_loss [mean, std]: ', metrics_summary_iRF['hamming_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets look the stability of feature importances across trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, feature importances are measured for the last forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top five feature importances across trials\n",
      "trial0:  [22 27 20 23  3]\n",
      "trial1:  [27 22  7 23 20]\n",
      "trial2:  [27 22 23 20  7]\n",
      "trial3:  [27  7 22 20 23]\n",
      "trial4:  [22 27 20 23  7]\n"
     ]
    }
   ],
   "source": [
    "print('top five important features across trials')\n",
    "\n",
    "for i in range(n_trials): \n",
    "    importances_rank = np.argsort(feature_importances_iRF[i])[::-1]\n",
    "    print('trial' + str(i) + ': ', importances_rank[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, lets examine the discovered interactions across trials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top five stable interactions across trials\n",
      "trial0:  ['27', '22_27', '22', '13_27', '13_22_27']\n",
      "trial1:  ['22', '22_27', '27', '7', '7_22']\n",
      "trial2:  ['27', '22_27', '22', '23_27', '22_23_27']\n",
      "trial3:  ['27', '7', '22', '22_27', '7_27']\n",
      "trial4:  ['27', '22_27', '22', '23_27', '20_27']\n"
     ]
    }
   ],
   "source": [
    "print('top five stable interactions across trials')\n",
    "\n",
    "for i in range(n_trials): \n",
    "    \n",
    "    # sort by stability\n",
    "    stability = sorted(stability_all[i].values(), reverse=True)\n",
    "    interactions = sorted(stability_all[i], key=stability_all[i].get, reverse=True)\n",
    "\n",
    "    print('trial' + str(i) + ': ', interactions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
